{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Vikrant\n",
      "[nltk_data]     Yadav/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Vikrant\n",
      "[nltk_data]     Yadav/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  \\\n",
      "0  To compound the terrifying experience, the pas...   \n",
      "1  On Wednesday, Ola CEO Bhavish Aggarwal announc...   \n",
      "2  A man recently took to social media to share h...   \n",
      "3  Ola will soon offer food and beverages through...   \n",
      "4  Welcome to a new edition of ETtech Unwrapped â€“...   \n",
      "\n",
      "                                     cleaned_content  \n",
      "0  compound terrifying experience passenger attem...  \n",
      "1  wednesday ola ceo bhavish aggarwal announced c...  \n",
      "2  man recently took social media share experienc...  \n",
      "3  ola soon offer food beverages throughout india...  \n",
      "4  welcome new edition ettech unwrapped weekend n...  \n",
      "   Topic  Count                                     Name  \\\n",
      "0     -1     41          -1_ola_company_ridehailing_cabs   \n",
      "1      0     13               0_maps_google_ola maps_ola   \n",
      "2      1     20          1_chief_hemant_officer_ola cabs   \n",
      "3      2     14       2_electric_ola electric_ola_shares   \n",
      "4      3     33               3_uber_market_strike_adani   \n",
      "5      4     32           4_cab_karnataka_bengaluru_post   \n",
      "6      5     13        5_ola_electric_authority_consumer   \n",
      "7      6     40  6_bhavish_aggarwal_ola_bhavish aggarwal   \n",
      "8      7     15     7_ipo_initial public_initial_sources   \n",
      "9      8     42   8_ettech_todays_morning_ettech morning   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [ola, company, ridehailing, cabs, india, new, ...   \n",
      "1  [maps, google, ola maps, ola, google maps, map...   \n",
      "2  [chief, hemant, officer, ola cabs, cabs, heman...   \n",
      "3  [electric, ola electric, ola, shares, ipo, deb...   \n",
      "4  [uber, market, strike, adani, drivers, workers...   \n",
      "5  [cab, karnataka, bengaluru, post, transport, a...   \n",
      "6  [ola, electric, authority, consumer, consumer ...   \n",
      "7  [bhavish, aggarwal, ola, bhavish aggarwal, fou...   \n",
      "8  [ipo, initial public, initial, sources, public...   \n",
      "9  [ettech, todays, morning, ettech morning, toda...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [ola cabs exit uk australia new zealand focus ...  \n",
      "1  [google introduced new features google maps in...  \n",
      "2  [titan capital founded kunal bahl rohit bansal...  \n",
      "3  [ola electrics ipo opens august retail subscri...  \n",
      "4  [gautam adanis adani group partnering uber ent...  \n",
      "5  [take look uniform fare dynamic pricing method...  \n",
      "6  [complaints registered nch authority asks plat...  \n",
      "7  [bloomberg two years ago bhavish aggarwal faci...  \n",
      "8  [india business news ola cabs planning initial...  \n",
      "9  [welcome new edition ettech unwrapped weekend ...  \n",
      "Topic 0:\n",
      "  maps: 0.1117\n",
      "\n",
      "Topic 1:\n",
      "\n",
      "Topic 2:\n",
      "\n",
      "Topic 3:\n",
      "\n",
      "Topic 4:\n",
      "\n",
      "Topic 5:\n",
      "\n",
      "Topic 6:\n",
      "\n",
      "Topic 7:\n",
      "\n",
      "Topic 8:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the CSV data from a file\n",
    "try:\n",
    "    # Replace 'input.csv' with the path to your CSV file\n",
    "    df = pd.read_csv('Olacabs_articles.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"The specified CSV file was not found.\")\n",
    "    exit()\n",
    "\n",
    "# Ensure the CSV has a 'content' column\n",
    "if 'content' not in df.columns:\n",
    "    print(\"The input CSV file must contain a 'content' column.\")\n",
    "    exit()\n",
    "\n",
    "# Define stopwords list (can be customized)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = re.sub(r'\\[\\+\\d+\\s*chars\\]', '', text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits (keeping only alphabets)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Rejoin tokens back into a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the tweet content\n",
    "df['cleaned_content'] = df['content'].apply(preprocess_text)\n",
    "\n",
    "# Display the cleaned content for verification\n",
    "print(df[['content', 'cleaned_content']].head())\n",
    "\n",
    "# Use a pre-trained model for sentence embeddings\n",
    "model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "# # Create and fit the BERTopic model\n",
    "# topic_model = BERTopic(\n",
    "#     embedding_model=model,       \n",
    "#     nr_topics=5         \n",
    "# )\n",
    "# topic_model.hdbscan_model.min_cluster_size = 5  # Adjust this to your preference\n",
    "# topic_model.hdbscan_model.min_samples = 2 \n",
    "\n",
    "# topics, _ = topic_model.fit_transform(df['cleaned_content'].tolist())\n",
    "\n",
    "# # View the topics\n",
    "# print(topic_model.get_topic_info())\n",
    "\n",
    "umap_model = UMAP(n_neighbors=10, min_dist=0.1, metric='cosine')  # Adjusting UMAP parameters for better clustering\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, 2))  # Use bigrams to capture short context\n",
    "\n",
    "# Create and fit the BERTopic model\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=model,        \n",
    "    umap_model=umap_model,       \n",
    "    vectorizer_model=vectorizer_model,\n",
    "    nr_topics=10,              # Let BERTopic decide the number of topics\n",
    "    # min_cluster_size=5,          # Minimum cluster size to capture meaningful topics\n",
    "    # min_samples=3                # Minimum samples per topic to avoid too small topics\n",
    ")\n",
    "topic_model.hdbscan_model.min_cluster_size = 10  # Adjust this to your preference\n",
    "topic_model.hdbscan_model.min_samples = 3 \n",
    "# Fit the model\n",
    "topics, _ = topic_model.fit_transform(df['cleaned_content'].tolist())\n",
    "\n",
    "# View the topics\n",
    "print(topic_model.get_topic_info())\n",
    "\n",
    "# Visualize the topics\n",
    "# topic_model.visualize_topics()\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# Save the topic information to a CSV file\n",
    "topic_info.to_csv('olaarticlestopics_output_news.csv', index=False)\n",
    "\n",
    "def filter_topic_terms(topic_model, score_threshold):\n",
    "    filtered_topics = {}\n",
    "    for topic_num in range(len(topic_model.get_topics())):\n",
    "        terms = topic_model.get_topic(topic_num)\n",
    "        if terms:  # Check if the topic has terms\n",
    "            filtered_terms = [(term, score) for term, score in terms if score >= score_threshold]\n",
    "            filtered_topics[topic_num] = filtered_terms\n",
    "    return filtered_topics\n",
    "\n",
    "# Set a threshold for term scores\n",
    "score_threshold = 0.1\n",
    "\n",
    "# Filter topics using the threshold\n",
    "filtered_topics = filter_topic_terms(topic_model, score_threshold)\n",
    "\n",
    "# Save the filtered topic info to a text file\n",
    "with open(\"olacabsarticles_topics_output.txt\", \"w\") as f:\n",
    "    for topic_num, terms in filtered_topics.items():\n",
    "        f.write(f\"Topic {topic_num}:\\n\")\n",
    "        for term, score in terms:\n",
    "            f.write(f\"  {term}: {score:.4f}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# Display the filtered topics\n",
    "for topic_num, terms in filtered_topics.items():\n",
    "    print(f\"Topic {topic_num}:\")\n",
    "    for term, score in terms:\n",
    "        print(f\"  {term}: {score:.4f}\")\n",
    "    print()\n",
    "# Save the final DataFrame to a CSV file\n",
    "# df.to_csv(\"filtered_tweet_topics_output.csv\", index=False)\n",
    "\n",
    "# print(\"Output has been saved to 'filtered_tweet_topics_output.csv' and 'filtered_topics_output.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topicmodels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
