{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Vikrant\n",
      "[nltk_data]     Yadav/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Vikrant\n",
      "[nltk_data]     Yadav/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  \\\n",
      "0  Yet again fucking driver accepted the booking ...   \n",
      "1  More than 1 hour and the food is still not her...   \n",
      "2  No one is constantly as motherfucking assholes...   \n",
      "3  Freelance content writers needed. Fully remote...   \n",
      "4  , Are you guys even concerned about what type ...   \n",
      "\n",
      "                                     cleaned_content            entities  \\\n",
      "0  yet fucking driver accepted booking came cance...                       \n",
      "1  hour food still olafoods handles delivery cust...  More than 1 hour #   \n",
      "2  one constantly motherfucking assholes driverst...          20 minutes   \n",
      "3  freelance content writers needed fully remote ...                 two   \n",
      "4  guys even concerned type people hiring booked ...               today   \n",
      "\n",
      "                                    combined_content  \n",
      "0  yet fucking driver accepted booking came cance...  \n",
      "1  hour food still olafoods handles delivery cust...  \n",
      "2  one constantly motherfucking assholes driverst...  \n",
      "3  freelance content writers needed fully remote ...  \n",
      "4  guys even concerned type people hiring booked ...  \n",
      "   Topic  Count                                               Name  \\\n",
      "0     -1    721                          -1_ola_hiring_service_amp   \n",
      "1      0   1285                        0_ola_service_hiring_layoff   \n",
      "2      1    506                             1_ola_launch_ev_stores   \n",
      "3      2    232                       2_merger_uber_ola uber_india   \n",
      "4      3    208                                   3_hai_ki_nahi_ho   \n",
      "5      4    119                         4_share_dm_contact_details   \n",
      "6      5    105               5_pune mumbai_pune_mumbai_outstation   \n",
      "7      6     45                  6_green_green bharat_share_bharat   \n",
      "8      7     13        7_bhai negotiation_wtf bhai_wtf_negotiation   \n",
      "9      8     10  8_celebrating person_best celebrating_blah_sho...   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [ola, hiring, service, amp, share, launch, sco...   \n",
      "1  [ola, service, hiring, layoff, electric, ola e...   \n",
      "2  [ola, launch, ev, stores, electric, india, rev...   \n",
      "3  [merger, uber, ola uber, india, ola, talks, me...   \n",
      "4      [hai, ki, nahi, ho, aur, ke, se, ola, el, ko]   \n",
      "5  [share, dm, contact, details, number, crn, hel...   \n",
      "6  [pune mumbai, pune, mumbai, outstation, avoid ...   \n",
      "7  [green, green bharat, share, bharat, network, ...   \n",
      "8  [bhai negotiation, wtf bhai, wtf, negotiation,...   \n",
      "9  [celebrating person, best celebrating, blah, s...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [ridehailing major ola started layoff employee...  \n",
      "1  [ola electric ipo st ev company file ipo Ola E...  \n",
      "2  [day waiting invitation grand launch ola ev ca...  \n",
      "3  [perhaps merger , perhaps merger , indias ola ...  \n",
      "4  [ola har baar ek shandar scooty launch karta h...  \n",
      "5  [share , understand upsetting might help pleas...  \n",
      "6  [avoid using outstation rides pune mumbai noto...  \n",
      "7  [glad share mr president attending presents gr...  \n",
      "8  [ 98 126, wtf bhai negotiation , wtf bhai nego...  \n",
      "9  [mom really know best celebrating one person a...  \n",
      "Topic 0:\n",
      "\n",
      "Topic 1:\n",
      "\n",
      "Topic 2:\n",
      "  merger: 0.1439\n",
      "  uber: 0.1002\n",
      "  ola uber: 0.0661\n",
      "  india: 0.0577\n",
      "  ola: 0.0520\n",
      "\n",
      "Topic 3:\n",
      "  hai: 0.0742\n",
      "\n",
      "Topic 4:\n",
      "  share: 0.1392\n",
      "  dm: 0.0831\n",
      "  contact: 0.0552\n",
      "  details: 0.0550\n",
      "  number: 0.0534\n",
      "\n",
      "Topic 5:\n",
      "  pune mumbai: 0.1816\n",
      "  pune: 0.1791\n",
      "  mumbai: 0.1762\n",
      "  outstation: 0.1091\n",
      "  avoid using: 0.1061\n",
      "  aware despite: 0.1061\n",
      "  careful sorry: 0.1061\n",
      "  mumbai notorious: 0.1061\n",
      "  midway leave: 0.1061\n",
      "  notorious stopping: 0.1061\n",
      "\n",
      "Topic 6:\n",
      "  green: 0.0767\n",
      "  green bharat: 0.0631\n",
      "  share: 0.0623\n",
      "  bharat: 0.0620\n",
      "  network: 0.0535\n",
      "  greenenergy: 0.0532\n",
      "  palace: 0.0532\n",
      "  palace new: 0.0532\n",
      "  taj palace: 0.0532\n",
      "  taj: 0.0532\n",
      "\n",
      "Topic 7:\n",
      "  bhai negotiation: 0.4936\n",
      "  wtf bhai: 0.4936\n",
      "  wtf: 0.4540\n",
      "  negotiation: 0.4540\n",
      "  safe: 0.3566\n",
      "  bhai: 0.3255\n",
      "  irritation sahi: 0.2666\n",
      "  irritation: 0.2666\n",
      "  affected wtf: 0.2666\n",
      "  covidimpact affected: 0.2666\n",
      "\n",
      "Topic 8:\n",
      "  celebrating person: 0.1107\n",
      "  best celebrating: 0.1107\n",
      "  blah: 0.1107\n",
      "  shows way: 0.1107\n",
      "  know best: 0.1107\n",
      "  mom really: 0.1107\n",
      "  person shows: 0.1107\n",
      "  really know: 0.1039\n",
      "  mom: 0.1014\n",
      "  celebrating: 0.0900\n",
      "\n",
      "Output has been saved to 'filtered_articles_with_entities_output.csv' and 'filtered_topics_output_with_entities.txt'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import json\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the CSV data from a file\n",
    "# try:\n",
    "#     # Replace 'input.csv' with the path to your CSV file\n",
    "#     df = pd.read_csv('Olacabs_articles.csv')\n",
    "# except FileNotFoundError:\n",
    "#     print(\"The specified CSV file was not found.\")\n",
    "#     exit()\n",
    "\n",
    "# # Ensure the CSV has a 'content' column\n",
    "# if 'content' not in df.columns:\n",
    "#     print(\"The input CSV file must contain a 'content' column.\")\n",
    "#     exit()\n",
    "try:\n",
    "    with open('Ola.json', 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"Error decoding the JSON file: {e}\")\n",
    "    exit()\n",
    "except FileNotFoundError:\n",
    "    print(\"The specified JSON file was not found.\")\n",
    "    exit()\n",
    "\n",
    "df = pd.json_normalize(data)\n",
    "\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    text = re.sub(r'\\[\\+\\d+\\s*chars\\]', '', text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    \n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "   \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    \n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text for ent in doc.ents]\n",
    "    return ' '.join(entities)\n",
    "\n",
    "df['cleaned_content'] = df['content'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "df['entities'] = df['content'].apply(extract_entities)\n",
    "\n",
    "\n",
    "df['combined_content'] = df['cleaned_content'] + ' ' + df['entities']\n",
    "\n",
    "\n",
    "print(df[['content', 'cleaned_content', 'entities', 'combined_content']].head())\n",
    "\n",
    "\n",
    "model = SentenceTransformer(\"paraphrase-xlm-r-multilingual-v1\")\n",
    "\n",
    "umap_model = UMAP(n_neighbors=10, min_dist=0.1, metric='cosine')  \n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, 2))  # Use bigrams to capture short context\n",
    "\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=model,        \n",
    "    umap_model=umap_model,       \n",
    "    vectorizer_model=vectorizer_model,\n",
    "    nr_topics=10,              \n",
    ")\n",
    "\n",
    "topic_model.hdbscan_model.min_cluster_size = 10  \n",
    "topic_model.hdbscan_model.min_samples = 3 \n",
    "\n",
    "\n",
    "topics, _ = topic_model.fit_transform(df['combined_content'].tolist())\n",
    "\n",
    "\n",
    "print(topic_model.get_topic_info())\n",
    "\n",
    "\n",
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info.to_csv('olaJSon_with_entities.csv', index=False)\n",
    "\n",
    "\n",
    "def filter_topic_terms(topic_model, score_threshold):\n",
    "    filtered_topics = {}\n",
    "    for topic_num in range(len(topic_model.get_topics())):\n",
    "        terms = topic_model.get_topic(topic_num)\n",
    "        if terms:  \n",
    "            filtered_terms = [(term, score) for term, score in terms if score >= score_threshold]\n",
    "            filtered_topics[topic_num] = filtered_terms\n",
    "    return filtered_topics\n",
    "\n",
    "\n",
    "score_threshold = 0.05\n",
    "\n",
    "\n",
    "filtered_topics = filter_topic_terms(topic_model, score_threshold)\n",
    "\n",
    "\n",
    "with open(\"olaJSonwith_entities.txt\", \"w\") as f:\n",
    "    for topic_num, terms in filtered_topics.items():\n",
    "        f.write(f\"Topic {topic_num}:\\n\")\n",
    "        for term, score in terms:\n",
    "            f.write(f\"  {term}: {score:.4f}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "\n",
    "for topic_num, terms in filtered_topics.items():\n",
    "    print(f\"Topic {topic_num}:\")\n",
    "    for term, score in terms:\n",
    "        print(f\"  {term}: {score:.4f}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "df.to_csv(\"Jsonentities_output.csv\", index=False)\n",
    "\n",
    "print(\"Output has been saved to 'filtered_articles_with_entities_output.csv' and 'filtered_topics_output_with_entities.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topicmodels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
