{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Vikrant\n",
      "[nltk_data]     Yadav/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Vikrant\n",
      "[nltk_data]     Yadav/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  \\\n",
      "0  Yet again fucking driver accepted the booking ...   \n",
      "1  More than 1 hour and the food is still not her...   \n",
      "2  No one is constantly as motherfucking assholes...   \n",
      "3  Freelance content writers needed. Fully remote...   \n",
      "4  , Are you guys even concerned about what type ...   \n",
      "\n",
      "                                     cleaned_content  \n",
      "0  yet fucking driver accepted booking came cance...  \n",
      "1  hour food still olafoods handles delivery cust...  \n",
      "2  one constantly motherfucking assholes driverst...  \n",
      "3  freelance content writers needed fully remote ...  \n",
      "4  guys even concerned type people hiring booked ...  \n",
      "   Topic  Count                                               Name  \\\n",
      "0     -1    763                        -1_ola_share_service_launch   \n",
      "1      0    889                        0_hiring_ola_service_driver   \n",
      "2      1    829                      1_ola_launch_electric_scooter   \n",
      "3      2    245                     2_layoff_layoffs_employees_ola   \n",
      "4      3    229                       3_merger_uber_ola uber_talks   \n",
      "5      4    151        4_ride_outstation rides_rides_aware despite   \n",
      "6      5     76                                     5_la_que_en_el   \n",
      "7      6     29  6_olaelectric_cnbctvmarket_cnbctvmarket olaele...   \n",
      "8      7     17  7_olympiacosbc_paobc_olympiacosbc paobc_paobc ...   \n",
      "9      8     16                 8_boats_boat_olaboat_rescue people   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [ola, share, service, launch, amp, electric, t...   \n",
      "1  [hiring, ola, service, driver, drivers, custom...   \n",
      "2  [ola, launch, electric, scooter, ev, share, se...   \n",
      "3  [layoff, layoffs, employees, ola, olacabs, lay...   \n",
      "4  [merger, uber, ola uber, talks, ola, uber merg...   \n",
      "5  [ride, outstation rides, rides, aware despite,...   \n",
      "6     [la, que, en, el, los, para, por, se, del, al]   \n",
      "7  [olaelectric, cnbctvmarket, cnbctvmarket olael...   \n",
      "8  [olympiacosbc, paobc, olympiacosbc paobc, paob...   \n",
      "9  [boats, boat, olaboat, rescue people, rescue, ...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [watch share future plans gives guided tour co...  \n",
      "1  [guys hiring people service centre dont know t...  \n",
      "2  [day waiting invitation grand launch ola ev ca...  \n",
      "3  [layoffs began earlier week affected employees...  \n",
      "4  [merger, indias ola uber merger talks via merg...  \n",
      "5  [avoid using outstation rides pune mumbai noto...  \n",
      "6  [estamos en busca de dos personas para integra...  \n",
      "7  [olaelectric instead falling weak market becam...  \n",
      "8  [olympiacosbc paobc, olympiacosbc paobc, olymp...  \n",
      "9  [joke anymore sent boat rescue people olaboat,...  \n",
      "                                             content  topic\n",
      "0  Yet again fucking driver accepted the booking ...     -1\n",
      "1  More than 1 hour and the food is still not her...      0\n",
      "2  No one is constantly as motherfucking assholes...     -1\n",
      "3  Freelance content writers needed. Fully remote...      0\n",
      "4  , Are you guys even concerned about what type ...      0\n",
      "Output has been saved to 'tweet_topics_output.csv' and 'topics_output.txt'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the JSON data from a file\n",
    "try:\n",
    "    with open('Ola.json', 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"Error decoding the JSON file: {e}\")\n",
    "    exit()\n",
    "except FileNotFoundError:\n",
    "    print(\"The specified JSON file was not found.\")\n",
    "    exit()\n",
    "\n",
    "# Convert the loaded data to a DataFrame\n",
    "df = pd.json_normalize(data)\n",
    "\n",
    "# Define stopwords list (can be customized)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess tweet text\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    # Remove mentions (e.g., @username)\n",
    "    # text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove hashtags (optional, can be kept if needed for analysis)\n",
    "    # text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove non-ASCII characters (e.g., emojis)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    # Remove special characters and digits, keep only alphabets and spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Rejoin tokens back into a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the tweet content\n",
    "df['cleaned_content'] = df['content'].apply(preprocess_text)\n",
    "\n",
    "# Display the cleaned content for verification\n",
    "print(df[['content', 'cleaned_content']].head())\n",
    "\n",
    "# Use a pre-trained model for sentence embeddings\n",
    "model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "# Set the hyperparameters for BERTopic\n",
    "umap_model = UMAP(n_neighbors=5, min_dist=0.3, metric='cosine')  # Adjusting UMAP parameters for better clustering\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, 2))  # Use bigrams to capture short context\n",
    "\n",
    "# Create and fit the BERTopic model\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=model,        \n",
    "    umap_model=umap_model,       \n",
    "    vectorizer_model=vectorizer_model,\n",
    "    nr_topics=10,              # Let BERTopic decide the number of topics\n",
    "    # min_cluster_size=5,          # Minimum cluster size to capture meaningful topics\n",
    "    # min_samples=3                # Minimum samples per topic to avoid too small topics\n",
    ")\n",
    "topic_model.hdbscan_model.min_cluster_size = 10  # Adjust this to your preference\n",
    "topic_model.hdbscan_model.min_samples = 3 \n",
    "# Fit the model\n",
    "topics, _ = topic_model.fit_transform(df['cleaned_content'].tolist())\n",
    "\n",
    "# View the topics\n",
    "print(topic_model.get_topic_info())\n",
    "\n",
    "# Visualize the topics\n",
    "# topic_model.visualize_topics()\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# Save the topic information to a CSV file\n",
    "topic_info.to_csv('OLAtopic_output.csv', index=False)\n",
    "# Get the top words in each topic and save them to a text file\n",
    "with open(\"olatopics_output.txt\", \"w\") as f:\n",
    "    for topic_num in range(len(topic_model.get_topics())):\n",
    "        f.write(f\"Topic {topic_num}: {topic_model.get_topic(topic_num)}\\n\\n\")\n",
    "\n",
    "# Add the topics to the DataFrame\n",
    "df['topic'] = topics\n",
    "\n",
    "# Display the DataFrame with topics assigned\n",
    "print(df[['content', 'topic']].head())\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "df.to_csv(\"OLAtweet_topics_output.csv\", index=False)\n",
    "\n",
    "print(\"Output has been saved to 'tweet_topics_output.csv' and 'topics_output.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topicmodels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
