{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Vikrant\n",
      "[nltk_data]     Yadav/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Vikrant\n",
      "[nltk_data]     Yadav/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  \\\n",
      "0  India, now get tasty meals delivered right at ...   \n",
      "1  Inaugurating record-breaking 4,000 stores on C...   \n",
      "2  DASH brings your favorite meals to your doorst...   \n",
      "3  Yep, taking our commitment to to the next leve...   \n",
      "4  Were not stopping at cities. 4,000 Ola Stores ...   \n",
      "\n",
      "                                     cleaned_content  \n",
      "0  india get tasty meals delivered right doorstep...  \n",
      "1  inaugurating recordbreaking stores christmas t...  \n",
      "2  dash brings favorite meals doorstep minutes li...  \n",
      "3  yep taking commitment next level scaling food ...  \n",
      "4  stopping cities ola stores service centres rol...  \n",
      "   Topic  Count                                       Name  \\\n",
      "0     -1     24  -1_future_electric_savingswalascooter_ola   \n",
      "1      0    104   0_savingswalascooter_ola_electric_future   \n",
      "2      1     22          1_olacoins_rewards_ride_discounts   \n",
      "3      2      9                2_diwali_happy_weekend_food   \n",
      "4      3      4              3_done_servicing_aliens_ather   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [future, electric, savingswalascooter, ola, gr...   \n",
      "1  [savingswalascooter, ola, electric, future, ev...   \n",
      "2  [olacoins, rewards, ride, discounts, win, earn...   \n",
      "3  [diwali, happy, weekend, food, free, get, ola,...   \n",
      "4  [done, servicing, aliens, ather, well, better,...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [kudos ola making evs household name providing...  \n",
      "1  [savingswalascooter, savingswalascooter, power...  \n",
      "2  [olacoins sounds like great idea earn olacoins...  \n",
      "3  [light lamp joy positivity growth endless poss...  \n",
      "4   [ather better, servicing aliens, well done keep]  \n",
      "                                             content  topic\n",
      "0  India, now get tasty meals delivered right at ...      2\n",
      "1  Inaugurating record-breaking 4,000 stores on C...      2\n",
      "2  DASH brings your favorite meals to your doorst...     -1\n",
      "3  Yep, taking our commitment to to the next leve...      0\n",
      "4  Were not stopping at cities. 4,000 Ola Stores ...      0\n",
      "Output has been saved to 'tweet_topics_output.csv' and 'topics_output.txt'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the JSON data from a file\n",
    "try:\n",
    "    with open('ola_combined.json', 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"Error decoding the JSON file: {e}\")\n",
    "    exit()\n",
    "except FileNotFoundError:\n",
    "    print(\"The specified JSON file was not found.\")\n",
    "    exit()\n",
    "\n",
    "# Convert the loaded data to a DataFrame\n",
    "df = pd.json_normalize(data)\n",
    "\n",
    "# Define stopwords list (can be customized)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits (keeping only alphabets)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Rejoin tokens back into a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the tweet content\n",
    "df['cleaned_content'] = df['content'].apply(preprocess_text)\n",
    "\n",
    "# Display the cleaned content for verification\n",
    "print(df[['content', 'cleaned_content']].head())\n",
    "\n",
    "# Use a pre-trained model for sentence embeddings\n",
    "model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "# Create and fit the BERTopic model\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=model,       \n",
    "    nr_topics=5         \n",
    ")\n",
    "topic_model.hdbscan_model.min_cluster_size = 3  # Adjust this to your preference\n",
    "topic_model.hdbscan_model.min_samples = 2 \n",
    "\n",
    "topics, _ = topic_model.fit_transform(df['cleaned_content'].tolist())\n",
    "\n",
    "# View the topics\n",
    "print(topic_model.get_topic_info())\n",
    "\n",
    "# Visualize the topics (Optional: You can comment this out if you want to just save results)\n",
    "# topic_model.visualize_topics()\n",
    "\n",
    "# Get the top words in each topic and save them to a text file\n",
    "with open(\"topics_output.txt\", \"w\") as f:\n",
    "    for topic_num in range(len(topic_model.get_topics())):\n",
    "        f.write(f\"Topic {topic_num}: {topic_model.get_topic(topic_num)}\\n\\n\")\n",
    "\n",
    "# Add the topics to the DataFrame\n",
    "df['topic'] = topics\n",
    "\n",
    "# Display the DataFrame with topics assigned\n",
    "print(df[['content', 'topic']].head())\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "df.to_csv(\"tweet_topics_output.csv\", index=False)\n",
    "\n",
    "print(\"Output has been saved to 'tweet_topics_output.csv' and 'topics_output.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topicmodels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
